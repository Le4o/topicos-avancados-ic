{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_VAE_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyIP4r4L235HMChNa4P//F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Le4o/topicos-avancados-ic/blob/main/GAN_VAE_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_5JhEdUsy5R"
      },
      "source": [
        "%load_ext autoreload\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCrU7QuvxZ42"
      },
      "source": [
        "%autoreload 2\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd.variable import Variable\n",
        "from torchvision import transforms, datasets"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJLHNkurxapY"
      },
      "source": [
        "DATA_FOLDER = './Datasets/medical_mnist'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS8IR-ISx_Ww",
        "outputId": "94d2a916-6cce-4677-eb18-5065a9a4f4a8"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "# !cp drive/MyDrive/logger.py .\n",
        "# import logger\n",
        "%ls"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "ls: cannot open directory '.': Transport endpoint is not connected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3qSFrT7xeqS"
      },
      "source": [
        "def mnist_data():\n",
        "    compose = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "    out_dir = '{}/dataset'.format(DATA_FOLDER)\n",
        "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        },
        "id": "E-1GJxqxxf6x",
        "outputId": "8400d47b-90eb-4e26-a6de-d73f907d1f63"
      },
      "source": [
        "# Load data\n",
        "data = mnist_data()\n",
        "# Create loader with data, so that we can iterate over it\n",
        "data_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True)\n",
        "# Num batches\n",
        "num_batches = len(data_loader)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-25-bde4f94b37c8>\", line 2, in <module>\n",
            "    data = mnist_data()\n",
            "  File \"<ipython-input-24-16bb7a9e4896>\", line 7, in mnist_data\n",
            "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\", line 87, in __init__\n",
            "    self.download()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\", line 168, in download\n",
            "    os.makedirs(self.raw_folder, exist_ok=True)\n",
            "  File \"/usr/lib/python3.7/os.py\", line 213, in makedirs\n",
            "    makedirs(head, exist_ok=exist_ok)\n",
            "  File \"/usr/lib/python3.7/os.py\", line 213, in makedirs\n",
            "    makedirs(head, exist_ok=exist_ok)\n",
            "  File \"/usr/lib/python3.7/os.py\", line 213, in makedirs\n",
            "    makedirs(head, exist_ok=exist_ok)\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"/usr/lib/python3.7/os.py\", line 223, in makedirs\n",
            "    mkdir(name, mode)\n",
            "OSError: [Errno 107] Transport endpoint is not connected: './Datasets'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZVy9yq7xh0I"
      },
      "source": [
        "class DiscriminatorNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A three hidden-layer discriminative neural network\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(DiscriminatorNet, self).__init__()\n",
        "        n_features = 784\n",
        "        n_out = 1\n",
        "        \n",
        "        self.hidden0 = nn.Sequential( \n",
        "            nn.Linear(n_features, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.hidden1 = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.hidden2 = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            torch.nn.Linear(256, n_out),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden0(x)\n",
        "        x = self.hidden1(x)\n",
        "        x = self.hidden2(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "    \n",
        "def images_to_vectors(images):\n",
        "    return images.view(images.size(0), 784)\n",
        "\n",
        "def vectors_to_images(vectors):\n",
        "    return vectors.view(vectors.size(0), 1, 28, 28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5bOfNP3xk6K"
      },
      "source": [
        "class GeneratorNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A three hidden-layer generative neural network\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(GeneratorNet, self).__init__()\n",
        "        n_features = 100\n",
        "        n_out = 784\n",
        "        \n",
        "        self.hidden0 = nn.Sequential(\n",
        "            nn.Linear(n_features, 256),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.hidden1 = nn.Sequential(            \n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.hidden2 = nn.Sequential(\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        \n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(1024, n_out),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden0(x)\n",
        "        x = self.hidden1(x)\n",
        "        x = self.hidden2(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "    \n",
        "# Noise\n",
        "def noise(size):\n",
        "    n = Variable(torch.randn(size, 100))\n",
        "    if torch.cuda.is_available(): return n.cuda() \n",
        "    return n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50dyDTScxmmJ"
      },
      "source": [
        "discriminator = DiscriminatorNet()\n",
        "generator = GeneratorNet()\n",
        "if torch.cuda.is_available():\n",
        "    discriminator.cuda()\n",
        "    generator.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iucHgRoGxr8u"
      },
      "source": [
        "# Optimizers\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002)\n",
        "\n",
        "# Loss function\n",
        "loss = nn.BCELoss()\n",
        "\n",
        "# Number of steps to apply to the discriminator\n",
        "d_steps = 1  # In Goodfellow et. al 2014 this variable is assigned to 1\n",
        "# Number of epochs\n",
        "num_epochs = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52ZUCtsTxtgX"
      },
      "source": [
        "def real_data_target(size):\n",
        "    '''\n",
        "    Tensor containing ones, with shape = size\n",
        "    '''\n",
        "    data = Variable(torch.ones(size, 1))\n",
        "    if torch.cuda.is_available(): return data.cuda()\n",
        "    return data\n",
        "\n",
        "def fake_data_target(size):\n",
        "    '''\n",
        "    Tensor containing zeros, with shape = size\n",
        "    '''\n",
        "    data = Variable(torch.zeros(size, 1))\n",
        "    if torch.cuda.is_available(): return data.cuda()\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x89RNBAxu6I"
      },
      "source": [
        "def train_discriminator(optimizer, real_data, fake_data):\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # 1.1 Train on Real Data\n",
        "    prediction_real = discriminator(real_data)\n",
        "    # Calculate error and backpropagate\n",
        "    error_real = loss(prediction_real, real_data_target(real_data.size(0)))\n",
        "    error_real.backward()\n",
        "\n",
        "    # 1.2 Train on Fake Data\n",
        "    prediction_fake = discriminator(fake_data)\n",
        "    # Calculate error and backpropagate\n",
        "    error_fake = loss(prediction_fake, fake_data_target(real_data.size(0)))\n",
        "    error_fake.backward()\n",
        "    \n",
        "    # 1.3 Update weights with gradients\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Return error\n",
        "    return error_real + error_fake, prediction_real, prediction_fake\n",
        "\n",
        "def train_generator(optimizer, fake_data):\n",
        "    # 2. Train Generator\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Sample noise and generate fake data\n",
        "    prediction = discriminator(fake_data)\n",
        "    # Calculate error and backpropagate\n",
        "    error = loss(prediction, real_data_target(prediction.size(0)))\n",
        "    error.backward()\n",
        "    # Update weights with gradients\n",
        "    optimizer.step()\n",
        "    # Return error\n",
        "    return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "susKsMibxwFW"
      },
      "source": [
        "num_test_samples = 16\n",
        "test_noise = noise(num_test_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unxYkFtmxxV3"
      },
      "source": [
        "logger = Logger(model_name='VGAN', data_name='MNIST')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for n_batch, (real_batch,_) in enumerate(data_loader):\n",
        "\n",
        "        # 1. Train Discriminator\n",
        "        real_data = Variable(images_to_vectors(real_batch))\n",
        "        if torch.cuda.is_available(): real_data = real_data.cuda()\n",
        "        # Generate fake data\n",
        "        fake_data = generator(noise(real_data.size(0))).detach()\n",
        "        # Train D\n",
        "        d_error, d_pred_real, d_pred_fake = train_discriminator(d_optimizer,\n",
        "                                                                real_data, fake_data)\n",
        "\n",
        "        # 2. Train Generator\n",
        "        # Generate fake data\n",
        "        fake_data = generator(noise(real_batch.size(0)))\n",
        "        # Train G\n",
        "        g_error = train_generator(g_optimizer, fake_data)\n",
        "        # Log error\n",
        "        logger.log(d_error, g_error, epoch, n_batch, num_batches)\n",
        "\n",
        "        # Display Progress\n",
        "        if (n_batch) % 100 == 0:\n",
        "            display.clear_output(True)\n",
        "            # Display Images\n",
        "            test_images = vectors_to_images(generator(test_noise)).data.cpu()\n",
        "            logger.log_images(test_images, num_test_samples, epoch, n_batch, num_batches);\n",
        "            # Display status Logs\n",
        "            logger.display_status(\n",
        "                epoch, num_epochs, n_batch, num_batches,\n",
        "                d_error, g_error, d_pred_real, d_pred_fake\n",
        "            )\n",
        "        # Model Checkpoints\n",
        "        logger.save_models(generator, discriminator, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}